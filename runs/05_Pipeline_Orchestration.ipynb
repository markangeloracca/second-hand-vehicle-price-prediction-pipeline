{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "952b8c95-6e68-451b-939a-2346a6f48996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Phase 5: Pipeline Orchestration\n",
    "\n",
    "This notebook creates a complete sklearn pipeline and provides batch prediction functionality.\n",
    "\n",
    "Note: Using sklearn instead of PySpark ML for compatibility with Databricks Community Edition.\n",
    "Model is kept in memory (not saved to disk) due to DBFS write restrictions on free tier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24fdfc88-78f0-414d-a0fd-4e20ab871b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e396e48d-e3c0-47cd-b068-0651ff21ab01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b577da29-7b4e-4fbd-b6de-0fa0914507e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb4d0d5-da1f-4f9c-8a67-9bd83de09044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data tables\n",
    "TRAIN_TABLE_NAME = \"gold_vehicles_train\"\n",
    "TEST_TABLE_NAME = \"gold_vehicles_test\"\n",
    "\n",
    "# Feature columns\n",
    "CATEGORICAL_COLS = [\"Make\", \"BodyType\", \"FuelType\", \"Transmission\", \"Drivetrain\"]\n",
    "NUMERICAL_COLS = [\n",
    "    \"Kilometres\",\n",
    "    \"City\",\n",
    "    \"Highway\",\n",
    "    \"vehicle_age\",\n",
    "    \"avg_fuel_efficiency\",\n",
    "    \"engine_displacement\",\n",
    "    \"cylinder_count\",\n",
    "    \"model_frequency_log\",\n",
    "]\n",
    "TARGET_COL = \"Price\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e093447-f00c-43c7-8222-cb635c64c581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6d2734-35f4-4c8d-90df-47b40b110401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded: 15634 rows\nTraining features shape: (15634, 13)\n"
     ]
    }
   ],
   "source": [
    "# Load train data from Delta table\n",
    "train_spark_df = spark.table(TRAIN_TABLE_NAME)\n",
    "print(f\"Training data loaded: {train_spark_df.count()} rows\")\n",
    "\n",
    "# Convert to pandas\n",
    "feature_cols = CATEGORICAL_COLS + NUMERICAL_COLS + [TARGET_COL]\n",
    "train_df = train_spark_df.select(feature_cols).toPandas()\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_df[CATEGORICAL_COLS + NUMERICAL_COLS]\n",
    "y_train = train_df[TARGET_COL]\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a362f9-b061-4434-b1ab-f1cd4aa8e26d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Build Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d36780-26c3-4b7a-b8db-d8405496f0d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline created with stages:\n  1. ColumnTransformer (OneHotEncoder + StandardScaler)\n  2. RandomForestRegressor\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "            CATEGORICAL_COLS,\n",
    "        ),\n",
    "        (\"num\", StandardScaler(), NUMERICAL_COLS),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create complete pipeline with Random Forest (best performing model)\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"regressor\",\n",
    "            RandomForestRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Pipeline created with stages:\")\n",
    "print(\"  1. ColumnTransformer (OneHotEncoder + StandardScaler)\")\n",
    "print(\"  2. RandomForestRegressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a10168-4e85-4569-9cd0-7d574a3f9de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd37de2c-1f60-4d61-964f-2c753ed96d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete pipeline...\nPipeline training complete in 5.73 seconds\nModel stored in memory (trained_pipeline variable)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training complete pipeline...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"Pipeline training complete in {training_time:.2f} seconds\")\n",
    "\n",
    "# Note: Model is kept in memory. DBFS write is not available on Community Edition.\n",
    "print(\"Model stored in memory (trained_pipeline variable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0313ab1c-7384-4f1d-acca-a8126fd73b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Pipeline on Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d4b6a6a-4fff-4cd3-8e09-d120b385c30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions:\n Make  Actual_Price  Predicted_Price\nAcura       43880.0     38940.895730\nAcura       36486.0     35920.127376\nAcura       44599.0     39027.551537\nAcura       46989.0     38487.158502\nAcura       60899.0     62339.299502\n"
     ]
    }
   ],
   "source": [
    "# Test on a small sample from training data\n",
    "test_sample = X_train.head(5)\n",
    "sample_predictions = pipeline.predict(test_sample)\n",
    "\n",
    "print(\"Sample predictions:\")\n",
    "sample_results = test_sample[[\"Make\"]].copy()\n",
    "sample_results[\"Actual_Price\"] = y_train.head(5).values\n",
    "sample_results[\"Predicted_Price\"] = sample_predictions\n",
    "print(sample_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9d10c7-a89d-4228-b88a-6b1dfd7c63a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Batch Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c80a9e-9d75-45ae-a055-de25e3ae69cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def batch_predict(input_table_name, output_table_name, model=None):\n",
    "    \"\"\"\n",
    "    Perform batch predictions on new data.\n",
    "\n",
    "    Parameters:\n",
    "    - input_table_name: Name of the input Delta table\n",
    "    - output_table_name: Name of the output Delta table for predictions\n",
    "    - model: Trained sklearn pipeline (uses global 'pipeline' if not provided)\n",
    "    \"\"\"\n",
    "    # Use provided model or global pipeline\n",
    "    if model is None:\n",
    "        model = pipeline\n",
    "\n",
    "    print(f\"Loading input data from {input_table_name}...\")\n",
    "    input_spark_df = spark.table(input_table_name)\n",
    "    print(f\"Input rows: {input_spark_df.count()}\")\n",
    "\n",
    "    # Convert to pandas\n",
    "    input_df = input_spark_df.toPandas()\n",
    "\n",
    "    # Prepare features - MUST be in exact same order as during training\n",
    "    feature_cols = CATEGORICAL_COLS + NUMERICAL_COLS\n",
    "    X = input_df[feature_cols].copy()\n",
    "\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # Create output dataframe\n",
    "    output_df = input_df.copy()\n",
    "    output_df[\"prediction\"] = predictions\n",
    "    output_df[\"prediction_timestamp\"] = datetime.now()\n",
    "    output_df[\"model_version\"] = \"v1.0_sklearn\"\n",
    "\n",
    "    # Select output columns\n",
    "    output_cols = [\n",
    "        \"record_id\",\n",
    "        \"Make\",\n",
    "        \"Model\",\n",
    "        \"Year\",\n",
    "        \"Kilometres\",\n",
    "        \"Price\",\n",
    "        \"prediction\",\n",
    "        \"prediction_timestamp\",\n",
    "        \"model_version\",\n",
    "    ]\n",
    "    available_output_cols = [c for c in output_cols if c in output_df.columns]\n",
    "    output_df = output_df[available_output_cols]\n",
    "\n",
    "    # Convert back to Spark and save to Delta\n",
    "    output_spark_df = spark.createDataFrame(output_df)\n",
    "    output_spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "        output_table_name\n",
    "    )\n",
    "\n",
    "    print(f\"Predictions saved to '{output_table_name}' table\")\n",
    "    print(f\"Total predictions: {len(output_df)}\")\n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4109587-733a-4c8a-913f-fd5fed20656a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Batch Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5bd664-54e3-4f46-b0dd-a6c7400d173e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch prediction on test set...\nLoading input data from gold_vehicles_test...\nInput rows: 3827\nGenerating predictions...\nPredictions saved to 'vehicle_price_predictions' table\nTotal predictions: 3827\n\nSample predictions:\n record_id  Make Model  Year  Kilometres   Price   prediction       prediction_timestamp model_version\n         2 Acura   RDX  2019     33032.0 40888.0 55431.066417 2025-12-05 01:57:49.407402  v1.0_sklearn\n         6 Acura   RDX  2020     27800.0 49099.0 59081.510702 2025-12-05 01:57:49.407402  v1.0_sklearn\n         8 Acura   RDX  2020     60892.0 38495.0 38593.491441 2025-12-05 01:57:49.407402  v1.0_sklearn\n        13 Acura   MDX  2020     65684.0 40995.0 38925.868947 2025-12-05 01:57:49.407402  v1.0_sklearn\n        19 Acura   ILX  2017     95000.0 21950.0 22158.743041 2025-12-05 01:57:49.407402  v1.0_sklearn\n        23 Acura   TLX  2019     43150.0 33495.0 30181.809181 2025-12-05 01:57:49.407402  v1.0_sklearn\n        29 Acura   RDX  2018     98175.0 29800.0 34904.089102 2025-12-05 01:57:49.407402  v1.0_sklearn\n        35 Acura   TLX  2019    119000.0 31450.0 31770.175691 2025-12-05 01:57:49.407402  v1.0_sklearn\n        46 Acura   MDX  2017     97000.0 33777.0 29854.390934 2025-12-05 01:57:49.407402  v1.0_sklearn\n        47 Acura   MDX  2019    116802.0 33795.0 30115.593527 2025-12-05 01:57:49.407402  v1.0_sklearn\n"
     ]
    }
   ],
   "source": [
    "PREDICTIONS_TABLE_NAME = \"vehicle_price_predictions\"\n",
    "\n",
    "print(\"Running batch prediction on test set...\")\n",
    "predictions_result = batch_predict(\n",
    "    TEST_TABLE_NAME, PREDICTIONS_TABLE_NAME, model=pipeline\n",
    ")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(predictions_result.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b80b70-d955-44a4-97f8-ae272e97fe75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prediction Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da505b1-ce8d-4540-bf4c-38c0c447deeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nPREDICTION STATISTICS\n================================================================================\nMean Absolute Error: $8,807.53\nRoot Mean Squared Error: $20,905.91\nR2 Score: 0.8521\n\nActual Price Range: $1,500.00 - $829,987.00\nPredicted Price Range: $5,347.37 - $668,029.58\nAverage Actual Price: $46,857.95\nAverage Predicted Price: $46,627.60\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load predictions and calculate statistics\n",
    "predictions_df = spark.table(PREDICTIONS_TABLE_NAME).toPandas()\n",
    "\n",
    "# Calculate metrics\n",
    "actual = predictions_df[\"Price\"]\n",
    "predicted = predictions_df[\"prediction\"]\n",
    "\n",
    "mae = mean_absolute_error(actual, predicted)\n",
    "rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "r2 = r2_score(actual, predicted)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Mean Absolute Error: ${mae:,.2f}\")\n",
    "print(f\"Root Mean Squared Error: ${rmse:,.2f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"\\nActual Price Range: ${actual.min():,.2f} - ${actual.max():,.2f}\")\n",
    "print(f\"Predicted Price Range: ${predicted.min():,.2f} - ${predicted.max():,.2f}\")\n",
    "print(f\"Average Actual Price: ${actual.mean():,.2f}\")\n",
    "print(f\"Average Predicted Price: ${predicted.mean():,.2f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a609e17-369e-4d5d-9fe1-a3bfa70407a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d98bd95-1dba-4247-a506-949beecbe37a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Metadata:\n  model_storage: In-memory (Databricks Community Edition limitation)\n  created_timestamp: 2025-12-05T01:57:53.051646\n  categorical_features: ['Make', 'BodyType', 'FuelType', 'Transmission', 'Drivetrain']\n  numerical_features: ['Kilometres', 'City', 'Highway', 'vehicle_age', 'avg_fuel_efficiency', 'engine_displacement', 'cylinder_count', 'model_frequency_log']\n  target_variable: Price\n  model_type: RandomForestRegressor (sklearn)\n  model_params: {'n_estimators': 100, 'max_depth': 10, 'min_samples_leaf': 5}\n  training_time_seconds: 5.730247\n"
     ]
    }
   ],
   "source": [
    "# Pipeline metadata\n",
    "pipeline_metadata = {\n",
    "    \"model_storage\": \"In-memory (Databricks Community Edition limitation)\",\n",
    "    \"created_timestamp\": datetime.now().isoformat(),\n",
    "    \"categorical_features\": CATEGORICAL_COLS,\n",
    "    \"numerical_features\": NUMERICAL_COLS,\n",
    "    \"target_variable\": TARGET_COL,\n",
    "    \"model_type\": \"RandomForestRegressor (sklearn)\",\n",
    "    \"model_params\": {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 10,\n",
    "        \"min_samples_leaf\": 5,\n",
    "    },\n",
    "    \"training_time_seconds\": training_time,\n",
    "}\n",
    "\n",
    "print(\"Pipeline Metadata:\")\n",
    "for key, value in pipeline_metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c5554f2-f84b-46c9-9b5c-3122916cb56c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0601dc8a-5635-4040-bfcf-aba41bd1cd62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nPIPELINE ORCHESTRATION COMPLETE\n================================================================================\nPipeline model: Stored in memory (variable: pipeline)\nTest predictions table: vehicle_price_predictions\nBatch prediction function: batch_predict()\nModel type: sklearn RandomForestRegressor\nTest set RMSE: $20,905.91\nTest set R2: 0.8521\nCompletion timestamp: 2025-12-05 01:57:53.298417\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PIPELINE ORCHESTRATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Pipeline model: Stored in memory (variable: pipeline)\")\n",
    "print(f\"Test predictions table: {PREDICTIONS_TABLE_NAME}\")\n",
    "print(\"Batch prediction function: batch_predict()\")\n",
    "print(\"Model type: sklearn RandomForestRegressor\")\n",
    "print(f\"Test set RMSE: ${rmse:,.2f}\")\n",
    "print(f\"Test set R2: {r2:.4f}\")\n",
    "print(f\"Completion timestamp: {datetime.now()}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Pipeline_Orchestration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}